---
title: "Homework 4: Machine Learning"
author: "Patrick Hofbauer"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
options(scipen = 999) #disable scientific notation
library(tidyverse)
library(tidymodels)
library(GGally)
library(sf)
library(leaflet)
library(janitor)
library(rpart.plot)
library(here)
library(scales)
library(vip)
kjj
```

# The Bechdel Test

<https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/>

The [Bechdel test](https://bechdeltest.com) is a way to assess how women are depicted in Hollywood movies. In order for a movie to pass the test:

1.  It has to have at least two [named] women in it
2.  Who talk to each other
3.  About something besides a man

There is a nice article and analysis you can find here <https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/> We have a sample of 1394 movies and we want to fit a model to predict whether a film passes the test or not.

```{r read_data}

bechdel <- read_csv(here::here("data", "bechdel.csv")) %>% 
  mutate(test = factor(test)) 
glimpse(bechdel)


```

How many films fail/pass the test, both as a number and as a %?

```{r}
#counting pass/fail and getting percentages
bechdel %>% 
  
  group_by(test) %>% 
  
  summarise(count = n()) %>% 
  
  mutate(total = sum(count), prop = count/total)
```

772 films have failed the Bechdel test and 622 films have passed it. The represents a failure rate of 55.4%.

## Movie scores

```{r}

#plotting the IMDB scores
ggplot(data = bechdel, aes(
  x = metascore,
  y = imdb_rating,
  colour = test #grouping by pass/fail
)) +
  geom_point(alpha = .3, size = 3) +
  scale_colour_manual(values = c("tomato", "olivedrab")) + #changing the colors of the groups
  labs(
    x = "Metacritic score",
    y = "IMDB rating",
    colour = "Bechdel test"
  ) +
 theme_light() #changing the theme of the graph
```

There seems to be a strong positive correlation between IMDB ratings and Metacritic scores, For both pass and fail films.

# Split the data

```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(bechdel, # updated data
                           prop = 0.8, 
                           strata = test)

bechdel_train <- training(data_split) 
bechdel_test <- testing(data_split)
```

Check the counts and % (proportions) of the `test` variable in each set.

```{r}
#checking count in bechdel_train
bechdel_train %>% 
  
  group_by(test) %>% 
  
  summarise(count = n()) %>% 
  
  mutate(total = sum(count), prop = count/total)
```

There are a total of 1114 rows, of which 617 are categorized of failing the bechdel test, a share of 55%

```{r}
#checking count in bechdel_test
bechdel_test %>% 
  
  group_by(test) %>% 
  
  summarise(count = n()) %>% 
  
  mutate(total = sum(count), prop = count/total)
```

There are a total of 280 rows, of which 155 are categorized of failing the bechdel test, a share of 55%. This is the same share as in the training dataset, therefore, we can proceed.

## Feature exploration

## Any outliers?

```{r}
#plotting all columns by pass/fail to check for outliers
bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore) %>% 

    pivot_longer(cols = 2:6,
               names_to = "feature",
               values_to = "value") %>% 
  ggplot()+
  aes(x=test, y = value, fill = test)+
  coord_flip()+
  geom_boxplot()+
  facet_wrap(~feature, scales = "free")+
  theme_bw()+
  theme(legend.position = "none")+
  labs(x=NULL,y = NULL)

```

In all selected categories the variables are on average higher if the film fails the bechdel test. There are a few outliers, for example in film budget one of the films that failed the Bechdel test, had a budget over 40m. In IMDB ratings, a few films that failed the Bechdel test had very low ratings and should be classified as outliers. There are also films in both test categories that grossed an unusual amount, both domesticlly and internationally.

## Scatterplot - Correlation Matrix

Write a paragraph discussing the output of the following

```{r, warning=FALSE, message=FALSE}

#plotting the correlation matrix of the dataframe 
bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore)%>% #selecting the required variables
  ggpairs(aes(colour=test), alpha=0.2)+
  theme_bw()
```

First and foremost, for all variables, when compared by whether the films have passed or failed the Bechdel test, films, who failed the test tend to have on average higher values, as evidenced by the box plots in the first row. In terms of correlations, the strongest overall correlation is between international and domestic gross, which is expected because the film would normally also do well internationally if it does well nationally. The second strongest overall correlation is between metascores and IMDB scores, which is also expected because they are both critic scores. If we look at differences in correlations when taking pass/fail into account, we see the biggest difference in the correlation between metascores and domestic gross, where bechdel-failed-films have a correlation of 0.216 and passed films a correlation of 0.107.

## Categorical variables

Write a paragraph discussing the output of the following

```{r, warning=FALSE}
#proportion of pass/fail by genre
bechdel %>% 
  group_by(genre, test) %>%
  summarise(n = n()) %>% #.groups = "drop.last"
  mutate(prop = n/sum(n))
```

The tables analyses the passing of the Bechdel test based on film genres. Sci-Fi and Documentary films have a 100% failure rate. Action, Mystery and Horror movies are the next categories to have the highest failure rates. Only Musicals and Thrillers have a 100% pass rate.

```{r, warning=FALSE}

#proportion of pass/fail by age restriction
bechdel %>% 
  group_by(rated, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
```

The table assess the bechdel test results of films based on the age restriction rating. It seems that films with a tighter restriction have a higher failure rate in the bechdel test. There is however a slight exceptions for films with a G rating. The highest failure rate is for films with a NC-17 rating andf the lowest is for films with a PG-13 rating.

# Train first models. `test ~ metascore + imdb_rating`

```{r}
#setting up the base model for logistic regression 
lr_mod <- logistic_reg() %>% 
  set_engine(engine = "glm") %>% #choosing the engine that we run the regression with
  set_mode("classification") #choosing the classification mode

lr_mod

#setting up the base model for a decision tree
tree_mod <- decision_tree() %>% 
  set_engine(engine = "C5.0") %>% 
  set_mode("classification")

tree_mod 
```

```{r}

#fitting the logistic regression model to the required variables
lr_fit <- lr_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )

#fitting the decision tree model to the required variables
tree_fit <- tree_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )

lr_fit
```

## Logistic regression

```{r}
#testing for prediction matches
lr_fit %>%
  broom::tidy() #converting stat objects into tidy tibbles

lr_preds <- lr_fit %>%
  augment(new_data = bechdel_train) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0))


```

### Confusion matrix

```{r}
lr_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")


```

## Decision Tree

```{r}

#testing for prediction matches
tree_preds <- tree_fit %>%
  augment(new_data = bechdel) %>% #bechdel train?
  mutate(.pred_match = if_else(test == .pred_class, 1, 0)) 


```

```{r}
tree_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

## Draw the decision tree

```{r}
draw_tree <- 
    rpart::rpart(
        test ~ metascore + imdb_rating,
        data = bechdel_train, # uses data that contains both birth weight and `low`
        control = rpart::rpart.control(maxdepth = 5, cp = 0, minsplit = 10)
    ) %>% 
    partykit::as.party()
plot(draw_tree)

```

# Cross Validation

Run the code below. What does it return?

```{r}
set.seed(123)
bechdel_folds <- vfold_cv(data = bechdel_train, 
                          v = 10, 
                          strata = test)
bechdel_folds
```

The code creates 10 new random data splits (folds) from the training data that can be reused as resamples to refit our models and test if they keep the same accuracy.

## `fit_resamples()`

Trains and tests a resampled model.

```{r}
lr_fit <- lr_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )


tree_fit <- tree_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )
```

## `collect_metrics()`

Unnest the metrics column from a tidymodels `fit_resamples()`

```{r}

collect_metrics(lr_fit)
collect_metrics(tree_fit)


```

Both models have an accuracy of about 57% after the resampling.

```{r}
tree_preds <- tree_mod %>% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds,
    control = control_resamples(save_pred = TRUE) #<<
  )

# What does the data for ROC look like?
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail)
```

We have higher sensitivity for specificity levels at thresholds.

```{r}
# Draw the ROC
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail) %>% 
  autoplot()

```

The model has a higher true pass rate than true fail rate for prediction at similar levels.

# Build a better training set with `recipes`

## Collapse Some Categorical Levels

Do we have any `genre` with few observations? Assign genres that have less than 3% to a new category 'Other'

```{r}
#| echo = FALSE
bechdel %>% 
  count(genre) %>% 
  mutate(genre = fct_reorder(genre, n)) %>% 
  ggplot(aes(x = genre, 
             y = n)) +
  geom_col(alpha = .8) +
  coord_flip() +
  labs(x = NULL) +
  geom_hline(yintercept = (nrow(bechdel_train)*.03), lty = 3)+
  theme_light()
```

```{r}
movie_rec <-
  recipe(test ~ .,
         data = bechdel_train) %>%
  
  # Genres with less than 5% will be in a category called 'Other'
    step_other(genre, threshold = .03) 
```

## Before recipe

```{r}
#| echo = FALSE
bechdel_train %>% 
  count(genre, sort = TRUE)
```

## After recipe

```{r}
movie_rec %>% 
  prep() %>% #estimate the parameters required of the recipe
  bake(new_data = bechdel_train) %>% #apply the newly trained recipe
  count(genre, sort = TRUE)

```

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  
  #Adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set.
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  
  #Converts nominal data into numeric dummy variables
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  
  #Intelligently handles zero variance variables (variables that contain only a single value)
  step_zv(all_numeric(), -all_outcomes())  %>% 
  
  #Centers then scales numeric variable (mean = 0, sd = 1)
  step_normalize(all_numeric()) 

```

## `step_corr()`

Removes highly correlated variables

```{r}

#IMPORTANT! 

#I ran into problems when adapting the code from the other document in order to run model comparisons. The models wouldn't converge and fail. As a result, I limited the number of predictors in the recipe to four and was finally able to fit the models.

movie_rec <- recipe(test ~ metascore + imdb_rating + genre + budget_2013, data = bechdel_train) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) %>% 
  step_corr(all_predictors(), threshold = 0.75, method = "spearman") 



movie_rec
```

# Define different models to fit

```{r, warning=FALSE}
## Model Building

# 1. Pick a `model type`
# 2. set the `engine`
# 3. Set the `mode`: regression or classification

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

# Bundle recipe and model with `workflows`

```{r}

log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(movie_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow


## A few more workflows

tree_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(knn_spec)

```

HEADS UP

1.  How many models have you specified?

-   In total, we specified 5 different models.

2.  What's the difference between a model specification and a workflow?

-   The model specification sets up the model before it can be trained. In the workflow we are adding the model and then any recipes that we created as preprocessors before training.

3.  Do you need to add a formula (e.g., `test ~ .`) if you have a recipe?

-   We actually can't add a formula if we add a recipe. The recipe already contains the necessary formula.

# Model Comparison

```{r}
log_res <- log_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
     recall, precision, f_meas, accuracy,
     kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 

# Show average performance over all folds (note that we use log_res):
log_res %>%  collect_metrics(summarize = TRUE)
```

Overall, we have an accuracy of 64.2% for this model.

```{r}
# Show performance for every single fold:
log_res %>%  collect_metrics(summarize = FALSE)
```

The folds have very differing accuracies.

```{r}
## `collect_predictions()` and get confusion matrix{.smaller}

log_pred <- log_res %>% collect_predictions()

log_pred %>%  conf_mat(test, .pred_class)
```

```{r}
#plotting confusion matrix with labels instead of numbers
log_pred %>% 
  conf_mat(test, .pred_class) %>% 
  autoplot(type = "mosaic") +
  geom_label(aes(
      x = (xmax + xmin) / 2, 
      y = (ymax + ymin) / 2, 
      label = c("TP", "FN", "FP", "TN")))
```

```{r}
#plotting confusion matrix normally
log_pred %>% 
  conf_mat(test, .pred_class) %>% 
  autoplot(type = "heatmap")

```

The model has a lower share of false negative predictions than false positive predictions.

```{r}
## ROC Curve

log_pred %>% 
  group_by(id) %>% # id contains our folds
  roc_curve(test, .pred_Pass) %>% 
  autoplot()
```

All folds seem to have higher specificity levels (better true fail prediction) for sensitivity levels.

```{r}
## Decision Tree results

tree_res <-
  tree_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

tree_res %>%  collect_metrics(summarize = TRUE)
```

This model has an accuracy of 62%

```{r}
## Random Forest

rf_res <-
  rf_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

rf_res %>%  collect_metrics(summarize = TRUE)
```

This model has an accuracy of 63.2%

```{r}
## Boosted tree - XGBoost

xgb_res <- 
  xgb_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

xgb_res %>% collect_metrics(summarize = TRUE)
```

This model has an accuracy of 61%

```{r}
## K-nearest neighbour

knn_res <- 
  knn_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

knn_res %>% collect_metrics(summarize = TRUE)
```

This model has an accuracy of 58.1%

```{r}
## Model Comparison

log_metrics <- 
  log_res %>% 
  collect_metrics(summarise = TRUE) %>%
  # add the name of the model to every row
  mutate(model = "Logistic Regression") 

tree_metrics <- 
  tree_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Decision Tree")

rf_metrics <- 
  rf_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest")

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost")

knn_metrics <- 
  knn_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Knn")

# create dataframe with all models
model_compare <- bind_rows(log_metrics,
                           tree_metrics,
                           rf_metrics,
                           xgb_metrics,
                           knn_metrics) 

#Pivot wider to create barplot
  model_comp <- model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean are under the curve (ROC-AUC) for every model
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), 
         y = mean_roc_auc + 0.08),
     vjust = 1
  )+
  theme_light()+
  theme(legend.position = "none")+
  labs(y = NULL)

```

In this comparison we can see that logistic regression has the highest accuracy of all trained models at 67%. Therefore, we will move forward with that model for the final fit.

```{r}
## `last_fit()` on test set

# - `last_fit()`  fits a model to the whole training data and evaluates it on the test set. 
# - provide the workflow object of the best model as well as the data split object (not the training data).


last_fit_log <- last_fit(log_wflow, 
                        split = data_split,
                        metrics = metric_set(
                          accuracy, f_meas, kap, precision,
                          recall, roc_auc, sens, spec))

last_fit_log %>% collect_metrics(summarize = TRUE)

#Compare to training
log_res %>% collect_metrics(summarize = TRUE)

```

Both models have a similar accuracy, so I'm moving forward with validating my training steps.

```{r, error=FALSE, warning=FALSE}
## Variable importance using `{vip}` package

library(vip)

last_fit_log %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  theme_light()
```

As we can see, IMDB rating is the most important variable to determine the failure of the bechdel test for a film.

```{r, error==FALSE}
## Final Confusion Matrix

last_fit_log %>%
  collect_predictions() %>% 
  conf_mat(test, .pred_class) %>% 
  autoplot(type = "heatmap")


## Final ROC curve
last_fit_log %>% 
  collect_predictions() %>% 
  roc_curve(test, .pred_Pass) %>% 
  autoplot()

```

The matrix predicts more True/False correctly than incorrectly. The model has a higher accuracy predicting the failure of the bechdel test rather than the passing.

# Details

-   Who did you collaborate with: Claudia Cerezo
-   Approximately how much time did you spend on this problem set: 4h
-   What, if anything, gave you the most trouble: Model Comparison, it just wouldn't run and now only works because I reduced the number of predictors for the formula

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

YES
